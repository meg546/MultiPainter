Instructions
Structure your deliverable according to the following sections. See the “Team Project Instructions” for details about formatting. Check the lecture materials and perform additional research to produce a high-quality deliverable. As usual, if you have any questions, let me know.

## 1. Description
Provide 1-2 paragraphs to describe your system. This will help us to remember what your system is about. 

Grading criteria (1 point): completeness, language.

## 2. Verification (tests)
Verification aims to ensure that you correctly developed the product.

 

#### Unit test

A unit test is an automated test that aims to verify the behavior of a component isolated from the rest of the system. For this deliverable, show an example of a unit test that uses mock objects to isolate the class from the rest of the system. 

Test framework you used to develop your tests (e.g., JUnit, unittest, pytest, etc.)
Link to your GitHub folder where your automated unit tests are located
An example of a test case that makes use of mock objects. Include in your answer a GitHub link to the class being tested and to the test
A print screen showing the result of the unit tests execution
Grading criteria (5 points): adequate choice of a test framework, coverage of the tests, quality of the tests, adequate use of Mock objects, and a print screen showing successful test execution.

 

#### Acceptance test

An acceptance test is a test that verifies the correct implementation of a feature from the user interface perspective. An acceptance test is a black box test (the system is tested without knowledge about its internal implementation). Provide the following information:

Test framework you used to develop your tests (e.g., Selenium, Katalon Studio, Espresso2, Cucumber, etc.)
Link to your GitHub folder where your automated acceptance tests are located
An example of an acceptance test. Include in your answer a GitHub link to the test and an explanation about the tested feature
A print screen/video showing the acceptance test execution
Grading criteria (7 points): adequate choice of a test framework, coverage of the tests, quality of the tests, adequate example of an acceptance test, print screen/video showing successful tests execution.

## 3. Validation (user evaluation)
Validation aims to ensure that you developed the right product. At the beginning of the semester, you talked to the clients/potential users to understand their needs. Now it is time to check if you are on the right track by conducting some user evaluation on the actual system. Include in this deliverable the following information:

### Script 

Here is our Script for validation!

#### **Task 1: Explore Website ( Expected time: 5 Mins)**

Task Description: Asked users to explore the website, and told them to try to look at
every page they can. Tell us when they think they found every page

**Collected Data**

[ How easy it is to navigate - what pages are useful - how we can improve navigation ]

**Questions Asked**

Q1) Did you have any difficulty finding particular pages? (I.E. free draw, templates,
community, sign-up, sign-in, home, or profile page). If so, what challenges in navigation
did you run into? Can you link it to a particular element?

A1)

Q2) How was your user experience? Did you find ease in navigation? Any particular
element or feature that was particularly helpful/confusing?

A2)

Q3) Which page did you find most engaging and useful? Which page did you find the
least engaging and least useful?

A3)

Q4) For navigation, do you have any further comments, suggestions for improvement, or
additional features that you feel could help mitigate confusion, enhance the user
experience, or increase the overall quality of the webpage?

A4)

#### **Task 2: Account Management (Expected time: 5 Mins)**

Task Description: Asked users to log in and view their profile. Provided no further
explanation on how to do this

**Collected Data**

[ Feedback on how sign in process works - First impressions of profile - Thoughts on the
issue with encryption ]

**Questions Asked**

Q1) How straightforward is the sign-up/sign-in/view profile process? Do you feel any
steps are confusing / are there steps where you encountered difficulties? If so, what
could we improve?

A1)

Q2) Upon viewing your profile what were your first impressions, Are there any features
you found most interesting? Are there any more implementations you would like to see
on these profiles?

A2)

Q3) On a scale of 1-10 how satisfied were you with the following [ Sign-up / Sign-in /
Profile ]

A3)

Q4) For the Sign-Up process, do you have any further comments, or suggestions for
improvement, or additional features that you feel could help mitigate confusion,
enhance the user experience, or increase the overall quality of the webpage?

A4)

#### **Task 3: Design Pixel Art (Expected Time: 10-20 mins)**

Task Description: The user is instructed To create 3 drawings, one of easy difficulty, one
of medium difficulty, and one of hard difficulty. User is asked to interact with as many
features on the free draw page as they feel fit.

**Collected Data**

[ User experience of drawing - User rating of easy/medium/difficult drawing - User’s
favored features ]

**Questions Asked**

Q1) How was your experience in creating art?

A1)

Q2) What was your favorite palette among the options provided? Why? How good is it
about the color picker?

A2)

Q3) How are the controls? Are they intuitive or confusing, could you find any bugs?
Would you change anything about how this page works?

A3)

Q4) For the drawing process, do you have any further comments, or suggestions for
improvement, or additional features that you feel could help mitigate confusion,
enhance the user experience, or increase the overall quality of the webpage?

A4)

#### **Task 4: Explore the Community & Work on a template (Expected Time 5-10 minutes)**

Task Description: The User is asked to browse the community page to their liking, and
when satisfied they are asked to choose a template and try to complete it. No further
instructions

**Data Collection**

[ Feedback on how the community page is styled - Feedback on how the art in the
community page is - Feedback on how hard it is to find the template - Feedback on how
the template mode works ]

**Questions Asked**

Q1) As you explored the community page, what were your initial impressions of the
artwork? The layout? The order in which you saw the art pieces?

A1)

Q2) How do you feel about the search functionality? Is there anything you feel we
should change about it?

A2)

Q3) How was the process of selecting a painting from the community page for work on
as a template? How was the actual template drawing experience?

A3)

Q4) For the Template process, do you have any further comments, or suggestions for
improvement, or additional features that you feel could help mitigate confusion,
enhance the user experience, or increase the overall quality of the webpage?

A4)

#### **Final Closing Questions (Expected time 5-10 mins)**

Q1) Based on your experience with the Multipixel website prototype, how satisfied are
you with the overall user experience?

A1)

Q2) On a scale of 1 to 10, how likely are you to recommend Mutipixel to your friends or
colleagues? What factors influenced your rating?

A2)

Q3) Did Mutipixel meet your expectations in terms of providing a social and artistic break
for you and your friends, as described in the initial value proposition?

A3)

Q4) Which features or functionalities did you find most essential or valuable during your
interaction with Mutipixel?

A4)

Q5) Were there any must-have features or functionalities that you felt were missing or
could be improved upon?

A5)

Q6) Did you notice any security, performance, portability, availability, or
maintainability issues while using Mutipixel? If so, please elaborate.

A6)

Q7) Are there any specific areas where you feel Mutipixel could differentiate itself further
from competitors or better meet the needs of its target audience?

Q7)

### Results: We conducted the user evaluation with 3 users so far. 

We interviewed someone experienced in art who has used the website before, who will be called User 1, Someone not experienced in art who has not used the website before, who will be called User 2, and someone very experienced in art and digital art, who has never used the website before, who will be called User 3.

**Here is the data we collected**

#### For Task 1

All Users found it easy to navigate the website and found almost all pages, however no users found the Sign-Up page. When asked about this, User 1 said they didn't feel they needed to do that yet so they never visited the page, and User 2 said they didn't even think about it.

There was a consensus that the nav bar was the most useful tool in navigation and all users liked it. Both User 2 and User 3 felt confusion on the home page, as the main page buttons lead the the same places as the nav bar. This is likely a place we will rework for the website. User 3 also pointed out that the "template" button leads to the "paint" tab, which is confusing.

For the most interesting and engaging page, the users agreed that free-draw and community were the most engaging, and also agreed that the main page and the template were the least engaging. This is a good example of places for us to work on for the next iteration of the website

For things to be implemented and improved. User 1 noted that it could add to the website overall if we cleaned up the "paint" page, and made its use more clear. User 2 felt it could be a nice easter egg if the buttons had SFX, or the cat meowed and got up and moved when you clicked on it. User 3 suggested we had more professional text on the home page, and had more information or picture on the home page to explain our website and vibes. They said it would be nice if we had an about us / about multipixel page on the website. A tutorial would also be nice they said.

#### For Task 2

#### For Task 3

#### For Task 4

#### For Final Questions

### Relfections

Reflections: Reflect on what you observed. Some questions that you can explore: What features worked well? What can be changed? How is the learning curve of your system? Did the users perform the tasks as you expected? Did the users’ actions produce the results they expected? What did the users like the most? Is your value proposition accomplished? 

Grading criteria (17 points): adequate script, adequate report of the results, adequate reflection, language.
